{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `contents.json` based on the `d2l-zh` repos, then manually copy and modify to `_modelules/*.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T05:11:03.099469Z",
     "start_time": "2021-09-14T05:11:01.340659Z"
    }
   },
   "outputs": [],
   "source": [
    "import notedown\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "entry = '''    {\n",
    "        \"title\":\"TITLE\",\n",
    "        \"day_break\":false,\n",
    "        \"book\":\"URL\",\n",
    "        \"slides\":[\"part-0.pdf\",0],\n",
    "        \"slides_video\":\"\",\n",
    "        \"notebook_video\":\"\",\n",
    "        \"qa_video\":\"\"\n",
    "    }'''\n",
    "\n",
    "\n",
    "book_repo = pathlib.Path('/Users/mli/repos/d2l-en')\n",
    "\n",
    "def get_toc(root):\n",
    "    \"\"\"return a list of files in the order defined by TOC\"\"\"\n",
    "    subpages = _get_subpages(root)\n",
    "    res = [root]\n",
    "    for fn in subpages:\n",
    "        res.extend(get_toc(fn))\n",
    "    return res\n",
    "\n",
    "def _get_subpages(input_fn):\n",
    "    \"\"\"read toc in input_fn, returns what it contains\"\"\"\n",
    "    subpages = []\n",
    "    reader = notedown.MarkdownReader()\n",
    "    with open(input_fn, 'r', encoding='UTF-8') as f:\n",
    "        nb = reader.read(f)\n",
    "    for cell in nb.cells:\n",
    "        if (cell.cell_type == 'code' and 'attributes' in cell.metadata and\n",
    "                'toc' in cell.metadata.attributes['classes']):\n",
    "            for l in cell.source.split('\\n'):\n",
    "                l = l.strip()\n",
    "                if not l.startswith(':'):\n",
    "                    fn = os.path.join(os.path.dirname(input_fn), l + '.md')\n",
    "                    if os.path.exists(fn):\n",
    "                        subpages.append(fn)\n",
    "    return subpages\n",
    "\n",
    "def _get_title(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            if l.startswith('#'): return l[1:].strip()\n",
    "\n",
    "entries = []\n",
    "notebooks = get_toc(str(book_repo/'index.md'))\n",
    "for nb in notebooks:\n",
    "    p = str(pathlib.Path(nb).relative_to(book_repo).with_suffix('.html'))\n",
    "    if 'index.md' in p: continue\n",
    "    title = _get_title(nb)\n",
    "    if not title: continue\n",
    "    entries.append(entry.replace('TITLE', title).replace('URL', p))\n",
    "\n",
    "with open('contents.json', 'w') as f:\n",
    "    f.write('[\\n' + ',\\n'.join(entries) + '\\n]\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T05:11:03.116157Z",
     "start_time": "2021-09-14T05:11:03.101253Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "import subprocess\n",
    "\n",
    "def extract_pdf(source, start_page, end_page, target):\n",
    "    source = pathlib.Path(source)\n",
    "    target = pathlib.Path(target)\n",
    "    if target.exists() and target.stat().st_mtime > source.stat().st_mtime:\n",
    "        return\n",
    "    pdf = PdfFileReader(str(source))\n",
    "    assert end_page > start_page\n",
    "    assert end_page <= pdf.getNumPages()\n",
    "    pdf_writer = PdfFileWriter()\n",
    "    for page in range(start_page, end_page):\n",
    "        pdf_writer.addPage(pdf.getPage(page))\n",
    "    with open('/tmp/tmp.pdf', 'wb') as out:\n",
    "        pdf_writer.write(out)\n",
    "    # compress pdf size\n",
    "    # refer to https://askubuntu.com/questions/113544/how-can-i-reduce-the-file-size-of-a-scanned-pdf-file\n",
    "    cmd = f'gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/prepress -dNOPAUSE  -dBATCH -sOutputFile={str(target)} /tmp/tmp.pdf'\n",
    "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n",
    "                               stderr=subprocess.PIPE)\n",
    "    stdout, _ = process.communicate()\n",
    "    if process.returncode != 0:\n",
    "        print(stdout.decode().splitlines())\n",
    "    print(f'Written {end_page-start_page} pages to {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all `_modules/part*.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T05:16:04.145250Z",
     "start_time": "2021-09-14T05:16:03.932538Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "pdf_dir = '/Users/mli/Google Drive/d2l-zh-v2-slides/'\n",
    "slides_dir = 'assets/pdfs/'\n",
    "notebooks_dir = 'assets/notebooks/'\n",
    "book_url = 'https://zh-v2.d2l.ai/'\n",
    "notebook_url = 'https://nbviewer.jupyter.org/format/slides/github/d2l-ai/d2l-zh-pytorch-slides/blob/main/'\n",
    "notbook_repo = '../d2l-zh-pytorch-slides/'\n",
    "video_url = 'https://www.bilibili.com/video/'\n",
    "cur_day = datetime.datetime(2021, 3, 19)\n",
    "\n",
    "titles = ['深度学习基础', '卷积神经网络', '计算机视觉', '循环神经网络', '注意力机制',]\n",
    "holidays = [datetime.datetime(2021, m, d) for m, d in ((4,3),(4,4),(4,25),(5,1),(5,2),(5,8),(5,9),(6,12),(6,13),(7,31),(8,1))]\n",
    "slide_pages = {}\n",
    "\n",
    "for i, title in enumerate(titles):\n",
    "    p = pathlib.Path('_modules')\n",
    "    with (p/f'part_{i}.md').open('w') as f:\n",
    "        f.write(f'---\\ntitle: {title}\\n---\\n')\n",
    "        if not (p/f'part_{i}.json').exists():\n",
    "            contents = []\n",
    "        else:\n",
    "            contents = json.load((p/f'part_{i}.json').open('r'))\n",
    "        for entry in contents:\n",
    "            if entry['day_break']:\n",
    "                while True:\n",
    "                    if cur_day.weekday() == 6: # sun\n",
    "                        cur_day += datetime.timedelta(days=6)\n",
    "                    else:\n",
    "                        cur_day += datetime.timedelta(days=1)\n",
    "                    f.write(f'\\n{cur_day.month}月{cur_day.day}日\\n\\n')\n",
    "                    if cur_day not in holidays:\n",
    "                        break\n",
    "                    f.write(': **休课**{: .label .label-green }\\n')\n",
    "            # title\n",
    "            f.write(f': {entry[\"title\"]}\\n')\n",
    "            # html page\n",
    "            if 'book' in entry and entry['book']:\n",
    "                f.write(f'  : [<span class=\"iconfont icon-xiaoshuo-copy\"></span>]({book_url+entry[\"book\"]})\\n')\n",
    "            else:\n",
    "                f.write('  : &nbsp; \\n')\n",
    "            # pdf\n",
    "            pdf, page = entry['slides']\n",
    "            if page:\n",
    "                if pdf not in slide_pages:\n",
    "                    slide_pages[pdf] = [0,]\n",
    "                save_pdf = f'{slides_dir}part-{i}_{len(slide_pages[pdf])}.pdf'\n",
    "                cur_page = sum(slide_pages[pdf])\n",
    "                extract_pdf(pdf_dir+pdf, cur_page, cur_page+page, save_pdf)\n",
    "                slide_pages[pdf].append(page)\n",
    "                f.write(f'  : [<span class=\"iconfont icon-KeynoteOutline\"></span>]({save_pdf})\\n')\n",
    "            else:\n",
    "                f.write('  : &nbsp; \\n')\n",
    "\n",
    "            # notebook\n",
    "            write_notebook = False\n",
    "            if 'book' in entry and entry['book'] and  (not 'notebook' in  entry or entry['notebook']):\n",
    "                notebook_path = entry[\"book\"].replace('.html', '.ipynb')\n",
    "                notebook_file = notbook_repo + notebook_path\n",
    "                notebook_output = pathlib.Path(notebooks_dir + notebook_path).with_suffix('.slides.html')\n",
    "                if os.path.exists(notebook_file):\n",
    "                    if not notebook_output.exists():\n",
    "                        os.system(f'jupyter nbconvert {notebook_file} --to slides --output-dir {str(notebook_output.parent)}')\n",
    "                    if notebook_output.exists():\n",
    "                        write_notebook = True\n",
    "                        f.write(f'  : [<span class=\"iconfont icon-jupyter\"></span>]({str(notebook_output)})\\n')\n",
    "            if not write_notebook:\n",
    "                f.write('  :  &nbsp; \\n')\n",
    "                    #if entry['notebook_video']:\n",
    "                    #    f.write(f' [<span style=\"font-size:130%\"  class=\"iconfont icon-bilibili-fill\"></span> 代码]({entry[\"notebook_video\"]}) &nbsp;')\n",
    "            #if 'qa_video' in entry and entry['qa_video']:\n",
    "            #   f.write(f' [<span style=\"font-size:130%\"  class=\"iconfont icon-bilibili-fill\"></span> 问答]({entry[\"qa_video\"]}) &nbsp;')\n",
    "            if entry['slides_video']:\n",
    "                f.write(f'  : [<span style=\"font-size:130%\"  class=\"iconfont icon-bilibili-fill\"></span>]({entry[\"slides_video\"]})\\n')\n",
    "            else:\n",
    "                f.write('  :  &nbsp; \\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "!touch index.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T05:57:54.653894Z",
     "start_time": "2021-07-16T05:57:54.648149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting video\n"
     ]
    }
   ],
   "source": [
    "%%writefile video\n",
    "6\t488\n",
    "14\t813\n",
    "2\t966\n",
    "5\t281\n",
    "17\t875\t1\n",
    "5\t446\t1\n",
    "14\t543\n",
    "19\t771\t1\n",
    "10\t758\n",
    "15\t699\n",
    "7\t402\t1\n",
    "11\t897\n",
    "6\t503\n",
    "8\t902\t1\n",
    "6\t402\t1\n",
    "11\t626\n",
    "7\t384\n",
    "6\t564\t1\n",
    "16\t1123\t1\n",
    "4\t249\t1\n",
    "11\t832\n",
    "13\t1359\n",
    "7\t505\t1\n",
    "7\t1116\n",
    "10\t1006\n",
    "8\t407\t1\n",
    "6\t781\n",
    "8\t754\t1\n",
    "6\t719\n",
    "6\t736\t1\n",
    "10\t902\n",
    "12\t1443\n",
    "13\t870\t1\n",
    "2\t519\n",
    "6\t807\t1\n",
    "10\t1039\t1\n",
    "4\t328\t1\n",
    "5\t363\t1\n",
    "8\t814\t1\n",
    "5\t998\n",
    "9\t661\n",
    "7\t899\n",
    "8\t607\t1\n",
    "8\t847\n",
    "3\t304\t1\n",
    "10\t1172\n",
    "3\t395\t1\n",
    "7\t620\n",
    "4\t417\n",
    "6\t510\n",
    "6\t1306\t1\n",
    "13\t2115\n",
    "4\t638\t1\n",
    "7\t543\n",
    "5\t442\t1\n",
    "6\t654\n",
    "4\t600\t1\n",
    "15\t1618\n",
    "4\t429\t1\n",
    "6\t1078\n",
    "7\t1031\t1\n",
    "9\t849\n",
    "6\t564\t1\n",
    "2\t396\n",
    "14\t2361\n",
    "16\t1758\n",
    "5\t1300\n",
    "5\t721\t1\n",
    "16\t1304\n",
    "9\t1238\n",
    "14\t768\t1\n",
    "9\t832\n",
    "8\t534\t1\n",
    "13\t1676\t1\n",
    "10\t1148\t1\n",
    "6\t695\n",
    "4\t227\t1\n",
    "6\t585\t1\n",
    "7\t1086\n",
    "18\t1400\t1\n",
    "16\t2524\n",
    "5\t596\t1\n",
    "14\t3165\t1\n",
    "5\t431\n",
    "10\t1676\t1\n",
    "3\t560\n",
    "5\t760\t1\n",
    "2\t262\n",
    "11\t1060\t1\n",
    "3\t297\n",
    "8\t1157\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T06:01:44.518853Z",
     "start_time": "2021-07-16T06:01:44.495121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 48\n",
      "avg page: 8.395833333333334\n",
      "avg length: 15.159722222222223\n",
      "page per min: 2.045208144478978\n",
      "total: 43\n",
      "avg page: 8.0\n",
      "avg length: 13.234883720930233\n",
      "page per min: 1.7313444384536163\n"
     ]
    }
   ],
   "source": [
    "with open('video') as f:\n",
    "    lines = f.readlines()\n",
    "    tokens = [[int(i) for i in l.strip().split('\\t')] for l in lines]\n",
    "nbs = [x for x in tokens if len(x)==2]\n",
    "code = [x[:2] for x in tokens if len(x)==3]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def analy(x):\n",
    "    x = np.array(x)\n",
    "    print('total:', len(x))\n",
    "    print('avg page:', x[:,0].mean())\n",
    "    print('avg length:', x[:,1].mean()/60)\n",
    "    print('page per min:', (x[:,1]/x[:,0]).mean()/60)\n",
    "\n",
    "analy(nbs)\n",
    "analy(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
