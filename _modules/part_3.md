---
title: 优化算法
---

5月23日

: Optimization Algorithms
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/index.html)

: Optimization and Deep Learning
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/optimization-intro.html)

: Convexity
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/convexity.html)

: Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/gd.html)

: Stochastic Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/sgd.html)

: Minibatch Stochastic Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/minibatch-sgd.html)


5月29日

: Momentum
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/momentum.html)

: Adagrad
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/adagrad.html)

: RMSProp
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/rmsprop.html)


5月30日

: Adadelta
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/adadelta.html)

: Adam
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/adam.html)

: Learning Rate Scheduling
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/lr-scheduler.html)

