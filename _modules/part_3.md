---
title: 优化算法
---

6月12日

: **长假无课**{: .label .label-green }

6月13日

: **长假无课**{: .label .label-green }

6月19日

: Optimization Algorithms
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/index.html)
: Optimization and Deep Learning
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/optimization-intro.html)
: Convexity
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/convexity.html)
: Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/gd.html)
: Stochastic Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/sgd.html)
: Minibatch Stochastic Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/minibatch-sgd.html)

6月20日

: Momentum
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/momentum.html)
: Adagrad
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/adagrad.html)
: RMSProp
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/rmsprop.html)

6月26日

: Adadelta
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/adadelta.html)
: Adam
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/adam.html)
: Learning Rate Scheduling
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_optimization/lr-scheduler.html)
