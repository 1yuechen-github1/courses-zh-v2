---
title: 优化算法
---

6月12日

: **长假无课**{: .label .label-green }

6月13日

: **长假无课**{: .label .label-green }

6月19日

: Optimization Algorithms
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/index.html) &nbsp;
: Optimization and Deep Learning
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/optimization-intro.html) &nbsp;
: Convexity
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/convexity.html) &nbsp;
: Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/gd.html) &nbsp;
: Stochastic Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/sgd.html) &nbsp;
: Minibatch Stochastic Gradient Descent
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/minibatch-sgd.html) &nbsp;

6月20日

: Momentum
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/momentum.html) &nbsp;
: Adagrad
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/adagrad.html) &nbsp;
: RMSProp
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/rmsprop.html) &nbsp;

6月26日

: Adadelta
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/adadelta.html) &nbsp;
: Adam
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/adam.html) &nbsp;
: Learning Rate Scheduling
  : [<span class="iconfont icon-xiaoshuo-copy"></span> 书](https://zh-v2.d2l.ai/chapter_optimization/lr-scheduler.html) &nbsp;
