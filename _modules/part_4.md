---
title: 注意力机制
---

8月1日

: 注意力机制
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)
  : [<span class="iconfont icon-KeynoteOutline"></span>](assets/pdfs/part-4_1.pdf)
  :  &nbsp; 
  :  &nbsp; 

: 注意力分数
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)
  : [<span class="iconfont icon-KeynoteOutline"></span>](assets/pdfs/part-4_2.pdf)
  :  &nbsp; 
  :  &nbsp; 

: 使用注意力机制的seq2seq
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)
  : [<span class="iconfont icon-KeynoteOutline"></span>](assets/pdfs/part-4_3.pdf)
  :  &nbsp; 
  :  &nbsp; 


8月7日

: 自注意力和位置编码
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)
  : [<span class="iconfont icon-KeynoteOutline"></span>](assets/pdfs/part-4_4.pdf)
  :  &nbsp; 
  :  &nbsp; 

: 多头注意力
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Transformer
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html)
  : [<span class="iconfont icon-KeynoteOutline"></span>](assets/pdfs/part-4_5.pdf)
  :  &nbsp; 
  :  &nbsp; 

