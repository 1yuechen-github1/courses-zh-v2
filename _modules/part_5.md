---
title: 注意力机制
---

7月4日

: Attention Mechanisms
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/index.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Attention Cues
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-cues.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Attention Pooling: Nadaraya-Watson Kernel Regression
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Attention Scoring Functions
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 


7月10日

: Bahdanau Attention
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Multi-Head Attention
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Self-Attention and Positional Encoding
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

: Transformer
  : [<span class="iconfont icon-xiaoshuo-copy"></span>](https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html)
  : &nbsp; 
  :  &nbsp; 
  :  &nbsp; 

